{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plyvel\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class RelatedItem(BaseModel):\n",
    "    title: str\n",
    "    url: str\n",
    "    html_content: str\n",
    "\n",
    "class NewsItem(BaseModel):\n",
    "    title: str\n",
    "    url: str\n",
    "    full_page_content: str\n",
    "    comes_from: str\n",
    "    published_time: str\n",
    "    comment_count: int\n",
    "    tags: List[str]\n",
    "    related: List[RelatedItem]\n",
    "\n",
    "\n",
    "def html_to_paragraphs(html: str):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    paragraphs = [p.get_text(strip=True) for p in soup.find_all('p') if p.get_text(strip=True)]\n",
    "    return paragraphs\n",
    "\n",
    "def save_paragraphs(paragraphs: list[str], count: int):\n",
    "    with open(f'extracted/{str(count).zfill(4)}.json', 'w+', encoding='utf-8') as f:\n",
    "        json.dump(paragraphs, f, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preparing...: 164703it [01:51, 1477.68it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preparing...: 57102it [00:40, 1542.52it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "news: list[list[str]] = []\n",
    "news_objects: list[NewsItem] = []\n",
    "with plyvel.DB('../../data/news_content.lvdb') as db:\n",
    "    \n",
    "    for k, v in tqdm(db.iterator(), desc=\"preparing...\"):\n",
    "        news_item = NewsItem.model_validate_json(str(v, encoding='utf-8'))\n",
    "        news_objects.append(news_item)\n",
    "        html = news_item.full_page_content\n",
    "        paragraphs = html_to_paragraphs(html)\n",
    "        news.append(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclean_http_links\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m re.sub(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps?://[^\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms]+\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, text)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m news = [[text_cleaner.clean_text(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m n] \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m(news, desc=\u001b[33m\"\u001b[39m\u001b[33mcleaning links\u001b[39m\u001b[33m\"\u001b[39m)]\n",
      "\u001b[31mNameError\u001b[39m: name 'tqdm' is not defined"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cleaning links:   9%|â–‰         | 15116/164703 [00:10<01:39, 1508.25it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclean_http_links\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m re.sub(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps?://[^\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms]+\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, text)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m news = [[\u001b[43mtext_cleaner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m n] \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m tqdm(news, desc=\u001b[33m\"\u001b[39m\u001b[33mcleaning links\u001b[39m\u001b[33m\"\u001b[39m)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/dream-lab/Development/Project/AIResearch/nur_cn_crawler/src/tools/text_cleaner.py:126\u001b[39m, in \u001b[36mclean_text\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclean_text\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    125\u001b[39m    text = clean_rare_symbols(text)\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m    text = \u001b[43mclean_extended_uyghur_characters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m    text = clean_chinese_text(text)\n\u001b[32m    128\u001b[39m    text = clean_unknown_symbols(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/dream-lab/Development/Project/AIResearch/nur_cn_crawler/src/tools/text_cleaner.py:107\u001b[39m, in \u001b[36mclean_extended_uyghur_characters\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m    105\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m replacement_table:\n\u001b[32m    106\u001b[39m     char = replacement_table[char]\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m   clean += char\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m clean\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import text_cleaner\n",
    "import re\n",
    "\n",
    "def clean_http_links(text: str) -> str:\n",
    "    return re.sub(r'https?://[^\\s]+', '', text)\n",
    "\n",
    "news = [[text_cleaner.clean_text(i) for i in n] for n in tqdm(news, desc=\"cleaning links\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "all_paragraphs: list[str] = []\n",
    "for p in news:\n",
    "    all_paragraphs.extend(p)\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        save_paragraphs(all_paragraphs, count)\n",
    "        all_paragraphs = []\n",
    "#save the last part of the paragraphs to json file.\n",
    "save_paragraphs(all_paragraphs, count)\n",
    "\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_symbols = set(\"\")\n",
    "for p in news[:]:\n",
    "    for s in p:\n",
    "        extracted_symbols.update(s)\n",
    "\n",
    "print(extracted_symbols)\n",
    "len(extracted_symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save all symbols to json file as tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [s for s in [text_cleaner.symbols_list]]\n",
    "with open('tokens.json', 'w+', encoding='utf-8') as f:\n",
    "    json.dump(tokens, f, ensure_ascii=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nur_cn_crawler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
